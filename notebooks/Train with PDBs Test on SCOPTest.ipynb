{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders import *\n",
    "from pdb_utils import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "from Bio import PDB\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle files/fold_groups.pkl', 'rb') as f:\n",
    "    fold_dict = pickle.load(f)\n",
    "with open('pickle files/label_dict.pkl', 'rb') as f:\n",
    "    label_dict = pickle.load(f)\n",
    "with open('pickle files/test_labels.pkl', 'rb') as f:\n",
    "    test_label_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "124\n"
     ]
    }
   ],
   "source": [
    "test_structures = []\n",
    "for pdb in os.listdir(\"SCOP_Test/\"):\n",
    "    pdb_path = os.path.join(\"SCOP_Test\", pdb)\n",
    "    parser = PDB.PDBParser()\n",
    "    structure = parser.get_structure(pdb, pdb_path)\n",
    "    test_structures.append(structure)\n",
    "\n",
    "print(len(test_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "626"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# OPTIONAL!! Filtering fold groups\n",
    "selected_folds = list(filter(lambda x: x[1]>2, map(lambda x: (x[0],len(x[1])), fold_dict.items())))\n",
    "len(selected_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3186\n"
     ]
    }
   ],
   "source": [
    "a=0\n",
    "for fold,pdb_list in fold_dict.items():\n",
    "    structures = []\n",
    "    if len(pdb_list)>=2: # for stratified sampling\n",
    "        for pdb in pdb_list[:5]: # at least 5 pdbs for each fold\n",
    "            a+=1\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "structures = []\n",
    "\n",
    "for fold,pdb_list in selected_folds:\n",
    "    for pdb in pdb_list[:5]:\n",
    "        pdb_path = os.path.join(\"PDBs\", pdb+\".pdb\")\n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(pdb, pdb_path)\n",
    "        structures.append(structure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Different strategies, filter sizes and encoding size\n",
    "\n",
    "results = {}\n",
    "for strategy in [\"strategy1\",\"strategy2\",\"strategy3\"]:\n",
    "    results[strategy] = []\n",
    "    for filter_size in [32,64,96,128,256]:\n",
    "        \n",
    "        structures = []\n",
    "        for fold,pdb_list in selected_folds:\n",
    "            for pdb in pdb_list[:5]:\n",
    "                pdb_path = os.path.join(\"PDBs\", pdb+\".pdb\")\n",
    "                parser = PDB.PDBParser()\n",
    "                structure = parser.get_structure(pdb, pdb_path)\n",
    "                sstructures.append(structure)\n",
    "        \n",
    "        if strategy == \"strategy1\":\n",
    "            # Resize\n",
    "            train_matrix = DistanceMatrixDict(structures, resize_strategy=\"strategy1\", resize_to=(filter_size,filter_size),removeSymmetry=True)\n",
    "            test_matrix = DistanceMatrixDict(test_structures, resize_strategy=\"strategy1\", resize_to=(filter_size,filter_size),removeSymmetry=True)\n",
    "            \n",
    "        elif strategy == \"strategy2\":\n",
    "            train_matrix = DistanceMatrixDict(structures, resize_strategy=\"strategy2\", resize_to=(filter_size,filter_size),sample_size=60)\n",
    "            test_matrix = DistanceMatrixDict(test_structures, resize_strategy=\"strategy2\", resize_to=(filter_size,filter_size),sample_size=60)\n",
    "\n",
    "        elif strategy == \"strategy3\":\n",
    "            train_matrix = DistanceMatrixDict(structures, resize_strategy=\"strategy3\", resize_to=(filter_size,filter_size))\n",
    "            test_matrix = DistanceMatrixDict(test_structures, resize_strategy=\"strategy3\", resize_to=(filter_size,filter_size))\n",
    "\n",
    "        for encoding_size in [50,100,200,500]:\n",
    "            train_pdb_names, train_features = list(train_matrix.keys()), list(train_matrix.values())\n",
    "            test_pdb_names, test_features = list(test_matrix.keys()), list(test_matrix.values())\n",
    "            input_size = len(train_features[0])\n",
    "            \n",
    "            # AutoEncoder\n",
    "            autoencoder = LinearAutoEncoder(input_size, encoding_size, n_iteration=100, learning_rate = 0.0001,model_path = \"models/autoencoder.ckpt\")\n",
    "            autoencoder.train(train_features)\n",
    "            \n",
    "            # Embedding vectors of train and test set\n",
    "            new_train_features = autoencoder.encode(train_features)\n",
    "            new_test_features = autoencoder.encode(test_features)      \n",
    "            \n",
    "            # Prepare train X and y\n",
    "            train_feature_dict = {}\n",
    "            for i in enumerate(train_pdb_names):\n",
    "                if \"sample\" in i[1]:\n",
    "                    pdb = i[1].split(\"sample\")[0]\n",
    "                else:\n",
    "                    pdb = i[1]\n",
    "                train_feature_dict.setdefault(pdb,[])\n",
    "                train_feature_dict[pdb].append(new_train_features[i[0]])\n",
    "                \n",
    "            # Preparing test X and y\n",
    "            test_feature_dict = {}\n",
    "            for i in enumerate(test_pdb_names):\n",
    "                if \"sample\" in i[1]:\n",
    "                    pdb = i[1].split(\"sample\")[0]\n",
    "                else:\n",
    "                    pdb = i[1]\n",
    "                test_feature_dict.setdefault(pdb,[])\n",
    "                test_feature_dict[pdb].append(new_test_features[i[0]])\n",
    "\n",
    "            X_train = []\n",
    "            y_train = []\n",
    "            \n",
    "            for pdb,vector in train_feature_dict.items():\n",
    "                X_train.append(np.average(vector,axis=0))\n",
    "                y_train.append(\".\".join(label_dict[pdb].split(\".\")[:2]))\n",
    "                \n",
    "            X_test = []\n",
    "            y_test = []\n",
    "            \n",
    "            for pdb,vector in test_feature_dict.items():\n",
    "                X_test.append(np.average(vector,axis=0))\n",
    "                y_test.append(\".\".join(test_label_dict[pdb.split(\".\")[0]].split(\".\")[:2]))\n",
    "                \n",
    "                \n",
    "            uniques = list(set(y_train).union(set(y_test)))\n",
    "            group2id = dict(zip(uniques, range(len(uniques))))\n",
    "\n",
    "            X_train = np.array(X_train)\n",
    "            y_train = np.array(list(map(lambda x: group2id[x], y_train)))\n",
    "            X_test = np.array(X_test)\n",
    "            y_test = np.array(list(map(lambda x: group2id[x], y_test)))\n",
    "\n",
    "\n",
    "            sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.40, random_state=42)\n",
    "            for a, b in sss1.split(X_train, y_train):\n",
    "                X_train_, y_train_ = X_train[a], y_train[a]\n",
    "                X_validation, y_validation = X_train[b], y_train[b]\n",
    "\n",
    "                # Hyperparameter Optimization with validation set\n",
    "                params = {'max_depth':[3,4,5,6,7,8,9,10,15,20], \n",
    "                      'criterion':('gini', 'entropy'), \n",
    "                      'warm_start':(True,False),\n",
    "                     'n_estimators': (10,50,100,200,500)}\n",
    "\n",
    "                rf = RandomForestClassifier(random_state=42)\n",
    "                clf = GridSearchCV(rf, params, cv=2, refit=True)\n",
    "                clf.fit(X_validation, y_validation)\n",
    "\n",
    "                # Training best model with train set\n",
    "                model = clf.best_estimator_\n",
    "                model.fit(X_train_, y_train_)\n",
    "\n",
    "                # Train and Test Accuracy Scores\n",
    "                train_acc = model.score(X_train_,y_train_)\n",
    "                test_acc = model.score(X_test,y_test)\n",
    "\n",
    "                # Saving Results\n",
    "                results[strategy].append(((filter_size,encoding_size,input_size), (train_acc,test_acc)))\n",
    "                print(((filter_size,encoding_size,input_size), (train_acc,test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle files/results.pkl', 'w') as f:\n",
    "    pickle.dump(f, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hacervenv",
   "language": "python",
   "name": "hacervenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
