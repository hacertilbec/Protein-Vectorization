{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from autoencoders import *\n",
    "from pdb_utils import *\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "from functools import partial\n",
    "import sys\n",
    "\n",
    "from Bio import PDB\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import *\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle files/fold_groups.pkl', 'rb') as f:\n",
    "    fold_dict = pickle.load(f)\n",
    "with open('pickle files/label_dict.pkl', 'rb') as f:\n",
    "    label_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_folds = list(filter(lambda x: x[1]>=10 x[1]<=30, map(lambda x: (x[0],len(x[1])), fold_dict.items())))[:5]\n",
    "len(selected_folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_structures = []\n",
    "for pdb in os.listdir(\"SCOP_Test/\"):\n",
    "    pdb_path = os.path.join(\"SCOP_Test\", pdb)\n",
    "    parser = PDB.PDBParser()\n",
    "    structure = parser.get_structure(pdb, pdb_path)\n",
    "    test_structures.append(structure)\n",
    "\n",
    "print(len(test_structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_time = time.time()\n",
    "structures = []\n",
    "\n",
    "for fold,_ in selected_folds:\n",
    "    for pdb in fold_dict[fold]:\n",
    "        pdb_path = os.path.join(\"PDBs\", pdb+\".pdb\")\n",
    "        parser = PDB.PDBParser()\n",
    "        structure = parser.get_structure(pdb, pdb_path)\n",
    "        structures.append(structure)\n",
    "end = time.time()\n",
    "nice_time(s_time,end)\n",
    "print(len(structures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {}\n",
    "for strategy in [\"strategy1\",\"strategy2\",\"strategy3\"]:\n",
    "    results[strategy] = []\n",
    "    for filter_size in [32,64,96,128,256]:\n",
    "        if strategy == \"strategy1\":\n",
    "            # Resize\n",
    "            matrixdict_s1 = DistanceMatrixDict(structures, resize_strategy=\"strategy1\", resize_to=(filter_size,filter_size),removeSymmetry=True)\n",
    "            \n",
    "        elif strategy == \"strategy2\":\n",
    "            matrixdict_s1 = DistanceMatrixDict(structures, resize_strategy=\"strategy2\", resize_to=(filter_size,filter_size),sample_size=60)\n",
    "        \n",
    "        elif strategy == \"strategy3\":\n",
    "            matrixdict_s1 = DistanceMatrixDict(structures, resize_strategy=\"strategy3\", resize_to=(filter_size,filter_size))\n",
    "\n",
    "        for encoding_size in [50,100,200,500]:\n",
    "            pdb_names, features = list(matrixdict_s1.keys()), list(matrixdict_s1.values())\n",
    "            input_size = len(features[0])\n",
    "            \n",
    "            # AutoEncoder\n",
    "            new_features, loss = LinearAutoencoder(features, input_size, encoding_size, 100, learning_rate=0.0001)\n",
    "            \n",
    "            # Preparing X and y\n",
    "            new_feature_dict = {}\n",
    "            for i in enumerate(pdb_names):\n",
    "                if \"sample\" in i[1]:\n",
    "                    pdb = i[1].split(\"sample\")[0]\n",
    "                else:\n",
    "                    pdb = i[1]\n",
    "                new_feature_dict.setdefault(pdb,[])\n",
    "                new_feature_dict[pdb].append(new_features[i[0]])\n",
    "\n",
    "            X = []\n",
    "            y_fold=[]\n",
    "            for pdb,vector in new_feature_dict.items():\n",
    "                X.append(np.average(vector,axis=0))\n",
    "                y_fold.append(\".\".join(label_dict[pdb].split(\".\")[:2]))\n",
    "            uniques = list(set(y_fold))\n",
    "            group2id = dict(zip(uniques, range(len(uniques))))\n",
    "            \n",
    "            y = np.array(list(map(lambda x: group2id[x], y_fold)))\n",
    "            X=np.array(X)\n",
    "            \n",
    "            # Fold Classification with Random Forest\n",
    "            train_acc = 0\n",
    "            test_acc = 0\n",
    "\n",
    "            sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\n",
    "            for train_index, test_index in sss.split(X, y):\n",
    "                X_test, y_test = X[test_index], y[test_index]\n",
    "\n",
    "                sss1 = StratifiedShuffleSplit(n_splits=1, test_size=0.25, random_state=42)\n",
    "                for a, b in sss1.split(X[train_index], y[train_index]):\n",
    "                    X_train, y_train = X[train_index][a], y[train_index][a]\n",
    "                    X_validation, y_validation = X[train_index][b], y[train_index][b]\n",
    "\n",
    "                # Hyperparameter Optimization with validation set\n",
    "                params = {'max_depth':[3,4,5,6,7,8,9,10,15,20], \n",
    "                      'criterion':('gini', 'entropy'), \n",
    "                      'warm_start':(True,False),\n",
    "                     'n_estimators': (10,50,100,200,500)}\n",
    "\n",
    "                rf = RandomForestClassifier(random_state=42)\n",
    "                clf = GridSearchCV(rf, params, cv=2, refit=True)\n",
    "                clf.fit(X_validation, y_validation)\n",
    "\n",
    "                # Training best model with train set\n",
    "                model = clf.best_estimator_\n",
    "                model.fit(X_train, y_train)\n",
    "\n",
    "                # Train and Test Accuracy Scores\n",
    "                train_acc+= model.score(X_train,y_train)\n",
    "                test_acc += model.score(X_test,y_test)\n",
    "\n",
    "            train_acc = train_acc/5.\n",
    "            test_acc = test_acc/5.\n",
    "            \n",
    "            \n",
    "            # Saving Results\n",
    "            results[strategy].append(((filter_size,encoding_size,input_size), (train_acc,test_acc)))\n",
    "            print(((filter_size,encoding_size,input_size), (train_acc,test_acc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('pickle files/results.pkl', 'w') as f:\n",
    "    pickle.dump(f, results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
